% !TEX encoding = UTF-8 Unicode
%\documentclass[a4paper,11pt,spanish, twoside, openany]{tfg-uam-matematicas_25-26} 
\documentclass[a4paper,11pt,spanish, twoside]{tfg-uam-matematicas_25-26} 

\usepackage[utf8]{inputenc}
\usepackage{amsfonts, amssymb, amsmath, amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{csquotes}

\usepackage[backend=biber,style=alphabetic]{biblatex}
\addbibresource{references.bib}

\usepackage{tikz}
\usetikzlibrary{decorations.markings, positioning}



\newtheorem{teor}{Teorema}[chapter]
\newtheorem{lema}[teor]{Lema}
\newtheorem*{teorsin}{Teorema}


\theoremstyle{definition}
\newtheorem{defin}[teor]{Definición}

\title{Causalidad y Estadística}
\author{Ignacio Ildefonso de Miguel Ruano}
\curso{2025-2026}


%%%%%METADATOS: rellenar la info solicitada entre llaves
\usepackage{hyperref}
\hypersetup{
	pdfinfo={
            Title={ }, %Titulo del trabajo; ejemplo: Matematicas y desarrollo
            Author={ }, %Autor del trabajo. 
            Director1={ }, %Tutor1: en formato nombre.apellido, tal como aparece en la primera parte, antes de la arroba,  de su dirección de correo electrónico de la UAM; ejemplo: fernando.soria
            Director2={ }, %Tutor2: en formato nombre.apellido, tal como aparece en la primera parte, antes de la arroba,  de su dirección de correo electrónico de la UAM
            Ndirectores={ }, %Numero total de directores: 1 ó 2
            Tipo={TFG}, %no tocar
            Curso={2025-26}, %no tocar
            Palabrasclave={ },% Palabras clave del trabajo, separadas por comas y sin acentos ni espacios; ejemplo: morfismos, formas modulares, ecuaciones elipticas
				}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\frontmatter
% Agradecimientos (Opcional)
\cleardoublepage
\null
\vfill
\begin{flushright}
    \textit{A todos los que me quieren.}
\end{flushright}
\vfill
\cleardoublepage
%%Fin de agradecimientos

%%Resumen, en español y en inglés (Obligatorios ambos)
\begin{abstract}[spanish]
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo. Sed fringilla mauris sit amet nibh. Donec sodales sagittis magna. Sed consequat, leo eget bibendum sodales, augue velit cursus nunc,
\end{abstract}
\begin{abstract}[english]
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero \end{abstract}

\cleardoublepage

% Lista de símbolos (Opcional)
% Título con mismo estilo que \chapter
\begin{flushright}
\Huge\bf Lista de símbolos\\*[-.5\baselineskip]
\hrulefill
\end{flushright}


\begin{tabular}{ll}
$\in$ & Pertenece a \\
$\notin$ & No pertenece a \\
$\subseteq$ & Subconjunto de \\
$\subset$ & Subconjunto propio de \\
$\cup$ & Unión de conjuntos \\
$\cap$ & Intersección de conjuntos \\
$\emptyset$ & Conjunto vacío \\
$\mathbb{N}$ & Conjunto de números naturales \\
$\mathbb{Z}$ & Conjunto de números enteros \\
$\mathbb{Q}$ & Conjunto de números racionales \\
$\mathbb{R}$ & Conjunto de números reales \\
$\mathbb{C}$ & Conjunto de números complejos \\
$f: A \to B$ & Función de $A$ en $B$ \\
$\mathrm{Id}$ & Identidad \\
$\ker(f)$ & Núcleo de una aplicación \\
$\mathrm{Im}(f)$ & Imagen de una aplicación \\
$\|x\|$ & Norma de un vector \\
$\langle x,y \rangle$ & Producto interno \\
$|A|$ & Cardinal de $A$ \\
$\forall$ & Para todo \\
$\exists$ & Existe \\
$\Rightarrow$ & Implica \\
$\Leftrightarrow$ & Equivalencia (si y sólo si) \\
$\sum_{i=1}^n a_i$ & Suma finita \\
$\prod_{i=1}^n a_i$ & Producto finito \\
$\lim_{x \to a} f(x)$ & Límite de $f$ en $a$ \\
$f'(x)$ & Derivada de $f$ \\
$\int_a^b f(x)\,dx$ & Integral definida \\
$O(g(n))$ & Cota asintótica superior \\
$\cong$ & Isomorfismo \\
$\simeq$ & Equivalencia \\
$\approx$ & Aproximadamente igual \\
$\triangleq$ & Definido como \\
$\mathbb{P}(A)$ & Probabilidad del suceso $A$ \\
$\mathbb{E}[X]$ & Esperanza de $X$ \\
$\mathrm{Var}(X)$ & Varianza de $X$ \\
\end{tabular}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mainmatter

%%%A partir de aquí el cuerpo del trabajo  %%%%%%%

\chapter{Introducción}\label{chap1}



\section{El problema de correlación vs. causalidad}
La estadística estándar utiliza conceptos asociacionales entre variables (correlación, dependencia, regresión\ldots). Estos conceptos se definen en base a una distribución conjunta de variables, $P(x_1, x_2, \ldots)$. El problema es que las relaciones causales no son definibles únicamente con una distribución de variables conjunta. Supongamos que $X$ e $Y$ son variables aleatorias de Bernoulli, cualesquiera que sean sus interpretaciones. Supongamos que observamos los datos de~\ref{tab:correlacion_simple}.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
         & \multicolumn{2}{c}{\textbf{$Y$}} & \\
        \cmidrule(lr){2-3}
        \textbf{$X$} & \textbf{Sí ($Y=1$)} & \textbf{No ($Y=0$)} & \textbf{Total} \\
        \midrule
        \textbf{Sí ($X=1$)} & \textbf{380} & 20 & 400 \\
        \textbf{No ($X=0$)} & 50 & \textbf{550} & 600 \\
        \midrule
        \textbf{Total} & 430 & 570 & 1000 \\
        \bottomrule
    \end{tabular}
    \caption{Tabla de observaciones de $X$ e $Y$}
    \label{tab:correlacion_simple}
\end{table}

Un cálculo rápido de las probabilidades condicionales nos lleva a que $P(X=1|Y=1) \approx 0.88$. Entonces, lo que podríamos decir es que, \textbf{dado que hemos observado $Y = 1$, es muy probable que hayamos observado $X=1$}. Es entonces cuando debemos resistirnos a afirmar que \textbf{$Y=1$ causa $X=1$}. Sin un `cuento semántico' que dé interpretaciones a las variables, las relaciones inferibles entre las variables no pueden ser causales \cite{DBLP:journals/corr/HuangV12}.

Supongamos que interpretamos $X$ como `el suelo está mojado' e $Y$ como `la gente lleva paraguas'. Ahora se vuelve más claro por qué no podemos afirmar que $X = 1$ cause $Y = 1$. Incluso existiendo una relación causal entre las variables, afirmaciones de este tipo pueden no ser ciertas: supongamos ahora que interpretamos $Y$ como `ha llovido'. Sabemos de sobra que $Y = 1$ causaría $X = 1$, pero si partimos de las probabilidades condicionales, sería indistinguible decir esto de decir que $X = 1$ causa $Y = 1$. Es decir, afirmar que llover causa que el suelo esté mojado porque las variables están correlacionadas sería tan válido como afirmar que el que el suelo esté mojado cause que llueva por la misma razón.

En definitiva, \textbf{$N$ variables no están causalmente relacionadas por estar correlacionadas}. Si volvemos a nuestra interpretación original de las variables $X$ e $Y$, sabemos de sobra que existe una variable $Z$ que podemos interpretar como `llueve', que \textbf{directamente} causa que el suelo esté mojado y que la gente lleve paraguas (y, en general, para cualquier par de variables $X$ e $Y$, si existe una correlación entre $X$ e $Y$ pero no hay relación causal, debe existir una variable $Z$ que explique esa correlación entre ambas~\cite{Reichenbach1956}). Esto ya nos lleva a deducir que \textbf{no es suficiente} con los razonamientos estadísticos estándar. No podemos inferir una relación causal si no asignamos un significado a las variables. Debemos elaborar un cuento causal, similar a lo que se vería en la Figura~\ref{fig:dag_causal}.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=2.5cm,
    decoration={markings, mark=at position 1 with {\arrow{stealth}}},
    every node/.style={circle, draw, minimum size=1cm, thick, fill=white}
]

% Nodos
\node (Z) {$Z$};
\node[below left=of Z] (X) {$X$};
\node[below right=of Z] (Y) {$Y$};

% Aristas
\draw[thick, postaction={decorate}] (Z) -- (X);
\draw[thick, postaction={decorate}] (Z) -- (Y);

\end{tikzpicture}
\caption{Grafo que representa las relaciones causales entre $X$, $Y$ y $Z$}
\label{fig:dag_causal}
\end{figure}

La diferencia clave entre la estadística tradicional y el estudio de la causalidad es la intervención. Una manera de saber si hay una relación causal entre dos variables es la de responder a la pregunta: \textbf{¿cambiaría la variable $X$ si modifico la variable $Y$?}, mientras que, originalmente, en estadística preguntamos \textbf{¿hemos observado que un cambio en $X$ está asociado a un cambio en la variable $Y$?}
\section{La paradoja de Simpson}
Supongamos una población en la que se ha descubierto una nueva epidemia. Rápidamente, los doctores responsables del bienestar de dicha población se ponen manos a la obra para intentar descubrir un remedio. Un estudio preliminar observacional sobre una medicación $M$ arroja los datos que podemos ver en la Tabla~\ref{tab:drug-no-gender}.

\begin{table}[ht]
    \centering
    \caption{Resultados de aplicar la medicación a la población descrita}
    \label{tab:drug-no-gender}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    & \textbf{Tomaron la medicina} & \textbf{No tomaron la medicina} \\
    \midrule
    Datos de curación  & 273 de 350 se curaron (78\%) & 289 de 350 se curaron (83\%) \\
    \bottomrule
    \end{tabular}
\end{table}

Considerando estos datos, lo más sensato sería retirar de inmediato la medicación, pues causa una tasa de curación menor. Sin embargo, nuevos datos revelan que \textbf{las mujeres son más vulnerables a la enfermedad que los hombres}. Se decide entonces separar por hombres y mujeres la tabla, para ver qué efectos tiene en ambos sexos por separado. La tabla resultante es la~\ref{tab:drug-gender}

\begin{table}[ht]
    \centering
    \caption{Resultados de aplicar la medicación a la población descrita, esta vez separando por sexo}
    \label{tab:drug-gender}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    & \textbf{Tomaron la medicina} & \textbf{No tomaron la medicina} \\
    \midrule
    Hombres            & 81 de 87 se curaron (93\%)   & 234 de 270 se curaron (87\%) \\
    Mujeres          & 192 de 263 se curaron (73\%) & 55 de 80 se curaron (69\%)   \\
    Datos de curación  & 273 de 350 se curaron (78\%) & 289 de 350 se curaron (83\%) \\
    \bottomrule
    \end{tabular}
\end{table}

Si llamamos $H$ a `ser hombre' y $C$ a `curarse', tenemos que, como antes:

\begin{equation*}
    P(C\mid M) < P(C\mid \neg M)
\end{equation*}

Pero, si condicionamos por el sexo, las desigualdades se invierten (inversión de Simpson, \cite{pearl2013simpson}):
\begin{align*}
    P(C\mid M, H) &> P(C\mid \neg M, H) \\
    P(C\mid M, \neg H) &> P(C\mid \neg M, \neg H)
\end{align*}

Es decir, al no observar el sexo del paciente, la probabilidad de curarse dado que se toma la medicación es menor que la de curarse sin tomarse nada, pero \textbf{una vez hemos observado el sexo del paciente}, concluimos que sí debe tomar la medicación. Carece de sentido lógico. A esto se le conoce como la Paradoja de Simpson (recibe su nombre de Edward Simpson ARTICULO).

Si nos centramos en los números de la tabla separada por sexo vemos que los grupos no son homogéneos. Hay más mujeres que tomaron la medicina que hombres. De igual manera, más hombres no tomaron la medicina que mujeres.

Volvemos a la afirmación anterior: las mujeres son más vulnerables que los hombres. Como más mujeres tomaron la medicina, la medicina tuvo que `enfrentarse' a casos más `difíciles' de resolver más frecuentemente. Es decir, en el caso de tomarse la medicina, esta se enfrentó más veces a casos más difíciles de curar. En el caso de no tomarla, la `no-medicina' se enfrentó mayoritariamente a casos fáciles de curar.

En este caso en particular, supongamos que se realiza una investigación en profundidad del criterio que utilizaron los médicos para recetar o no el remedio. Descubrimos que, ante la incertidumbre de saber si realmente surtiría un efecto positivo, lo emplearon con mayor tendencia en casos de mayor riesgo, en este caso los pacientes de sexo femenino.

Esto causó los datos que vemos en la tabla, donde el remedio se enfrentaba en su mayoría a casos de mujeres. Si ahora intentamos construir un grafo similar a la Figura~\ref{fig:dag_causal}, podemos establecer varias relaciones causales:

\begin{enumerate}
    \item La curación de un paciente depende de si se le da la medicina o no.
    \item La curación de un paciente depende de su sexo.
    \item El que se recete el remedio a un paciente depende de su sexo.
\end{enumerate}

Por tanto, el grafo que podríamos construir sería similar al de la Figura~\ref{fig:simpson-dag}. La tabla segregada y la agregada presentan conclusiones contradictorias, así pues, \textbf{¿qué tabla escoger para evaluar la efectividad del tratamiento?}

\begin{figure}[ht]
    \centering
\begin{tikzpicture}[
    node distance=2.5cm,
    decoration={markings, mark=at position 1 with {\arrow{stealth}}},
    every node/.style={circle, draw, minimum size=1cm, thick, fill=white}
]

% Nodos
\node (H) {$H$};
\node[below left=of H] (X) {$M$};
\node[below right=of H] (Y) {$C$};

% Aristas
\draw[thick, postaction={decorate}] (H) -- (X);
\draw[thick, postaction={decorate}] (H) -- (Y);
\draw[thick, postaction={decorate}] (X) -- (Y);

\end{tikzpicture}
    \caption{Diagrama Causal (DAG) de la Paradoja de Simpson. La variable $H$ (Sexo) tiene efecto tanto en la receta del tratamiento como en la curación.}
    \label{fig:simpson-dag}
\end{figure}

Al ser menos probable que una mujer se cure, independientemente de la medicación que tome, lo más razonable es separar los datos. De cualquier otra manera, nuestros datos presentan una `competición' de variables, donde el tratamiento es menos determinante que el sexo del individuo.

Lo que deberíamos hacer entonces es tratar de mirar las variables por separado, es decir, fijar las dos, o lo que es lo mismo, considerar las probabilidades condicionadas. Aquí es donde nuestra intuición es correcta: los datos más específicos nos dan la interpretación correcta.

La paradoja de Simpson también puede ocurrir en contexto de variables continuas. Supongamos que elaboramos un estudio observacional donde tratamos de averiguar cómo afecta un incremento en el poder adquisitivo al coste de vida relativo. Esperaríamos ver que, según aumenta el poder adquisitivo de una persona, disminuye el coste de vida relativo a su patrimonio.

Sin embargo, observamos un gráfico como el de la Figura~\ref{fig:aggregate-simpson-scatter}. Nuestras expectativas intuitivas se ven desafiadas por la aparente correlación que existe entre mayor patrimonio y mayor coste de vida relativo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{simpson-scatter-normal.png}
    \caption{Datos de comparación de poder adquisitivo agregados}
    \label{fig:aggregate-simpson-scatter}
\end{figure}

En el análisis de nuestra observación nos damos cuenta de que hemos comparado individuos de distintas ubicaciones geográficas de forma agregada. Si separamos los datos por dicha ubicación vemos un resultado distinto. Se ve en la Figura~\ref{fig:segregated-scatter}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{simpson-segregated-scatter.png}
    \caption{Datos de comparación, esta vez segregados}
    \label{fig:segregated-scatter}
\end{figure}

En este caso, la ubicación causa que el patrimonio sea más alto (pues para poder permitirse vivir en centros urbanos uno debe tener más dinero), pero también causa que el coste de vida sea más alto en comparación al patrimonio (esto da una situación muy similar a la de la Figura~\ref{fig:simpson-dag}. En cuanto comparamos los datos segregados descubrimos que nuestra intuición era correcta.

\section{Objetivos}

El objetivo de este trabajo es establecer una base de entendimiento de los métodos modernos para la expresión de modelos causales, complementando a la estadística tradicional. 

En particular, buscamos analizar las limitaciones de la estadística clásica para extraer relaciones causales, formalizar la teoría de los \textit{Directed Acyclic Graphs} (\textbf{DAGs}) en el contexto causal, definir con rigor los conceptos de \textit{d-separation} y \textbf{bloqueos}, establecer distinciones entre la probabilidad condicionada tradicional $P(Y \mid X)$ y $P(Y \mid do(X))$, presentar las reglas del $do$-Calculus, explorar los conceptos de \textit{Back-door criterion} y \textit{Front-door criterion}.

\chapter{Bases conceptuales}
\section{Conceptos de probabilidad}
A lo largo del documento, asumimos siempre que estamos en un espacio de probabilidad $(\Omega, \mathcal{F}, P)$, con $\Omega$ el espacio muestral, $\mathcal{F}$ su $\sigma-$álgebra y $P:\mathcal{F} \rightarrow \mathbb{R}$ la función de medida de probabilidad que satisface los tres axiomas de Kolmogorov.

\begin{defin}[Variable aleatoria]
Una variable aleatoria es una función $X:\Omega \rightarrow \mathbb{R}$ tal que es medible, es decir, que cada preimagen de $X$ sobre un elemento de la $\sigma-$álgebra de Borel es un elemento de $\mathcal{F}$.
\end{defin}

\begin{defin}[Probabilidad condicionada]
Sean $A$ y $B$ dos sucesos tales que $P(B) > 0$. Definimos la probabilidad de $A$ condicionada a $B$ como
\begin{equation*}
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
\end{equation*}
\end{defin}

\begin{defin}[Independencia condicional]
Decimos que las variables $X$ e $Y$ son condicionalmente independientes dado $Z$ si
\begin{equation*}
P(X, Y \mid Z) = P(X\mid Z)P(Y\mid Z)
\end{equation*}
\end{defin}

\begin{teor}[Ley de la probabilidad total]
Supongamos $I \subseteq \mathbb{N}$ un conjunto finito o numerable. Digamos que los conjuntos $B_i$ con $i \in I$ son tales que:
\begin{itemize}
    \item $B_i \in \mathcal{F}$ para todo $i \in I$.
    \item $i \neq j \rightarrow B_i \cap B_j = \emptyset$.
    \item $\bigcup_{i \in I} B_i = \Omega$.
    \item $P(B_i) > 0$ para todo $i \in I$.
\end{itemize}
Entonces, para cualquier suceso $A \in \mathcal{F}$:
\begin{equation*}
P(A) = \sum_{i \in I} P(A\mid B_i) P(B_i)
\end{equation*}
\end{teor}

\begin{proof}
Ver Apéndice~\ref{app:demo}
\end{proof}

\section{Teoría de grafos}
\begin{defin}[Grafo dirigido]
Decimos que el par $G = \langle V, E\rangle$ donde 
\begin{itemize}
    \item $V = \{X_1, X_2, \ldots, X_n \mid n \in \mathbb{N}\}$
    \item $E \subseteq V \times V$ es un subconjunto de los pares ordenados con elementos en $V$.
\end{itemize}
es un grafo dirigido con vértices $V$ y aristas $E$.
\end{defin}

\textbf{Notación:} para todo par $(X_i, X_j) \in E$, diremos $X_i \rightarrow X_j$

\begin{defin}[Parentesco]
Para todo vértice del grafo $X_i$, definimos
\begin{itemize}
\item Los \textbf{padres} de $X_i$: $\text{PA}(X_i)=\{X_j \in V \mid X_j \rightarrow X_i \}$
\item Los \textbf{hijos} de $X_i$: $\text{CH}(X_i)=\{X_j \in V \mid X_i \rightarrow X_j \}$
\end{itemize}
\label{def:parentesco}
\end{defin}

\begin{defin}[Camino]
Dado un grafo dirigido $G=(V,E)$, un \textbf{camino} (o sendero) entre dos nodos $X$ e $Y$ es una secuencia de vértices distintos $(V_0,V_1,\ldots,V_k)$ tal que:
\begin{enumerate}
\item $V_0=X$ y $V_k=Y$.
\item Para cada $i \in \{0,\ldots,k-1\}$, los vértices son adyacentes en $G$. Es decir, se cumple $(V_i,V_{i+1}) \in E$ o bien $(V_{i+1},V_i)\in E$.
\end{enumerate}
\end{defin}

\begin{defin}[Camino dirigido]
Dado un grafo dirigido $G=(V,E)$, un \textbf{camino dirigido} entre dos nodos $X$ e $Y$ es una secuencia de vértices distintos $(V_0,V_1,\ldots,V_k)$ tal que:
\begin{enumerate}
\item $V_0=X$ y $V_k=Y$.
\item Para cada $i \in \{0,\ldots,k-1\}$, los vértices están conectados en $G$. Es decir, se cumple $(V_i,V_{i+1}) \in E$.
\end{enumerate}
\end{defin}

\begin{defin}[Ciclo Dirigido]
Un \textbf{ciclo dirigido} es una secuencia de vértices $(V_0, V_1, \ldots, V_k)$ con $k \geq 1$ tal que:
\begin{enumerate}
    \item $(V_0, \ldots, V_{k-1})$ es un camino dirigido (vértices distintos).
    \item $V_0 = V_k$ (el vértice inicial coincide con el final).
\end{enumerate}
\end{defin}

\begin{defin}[Grafo Acíclico Dirigido - DAG]
Un grafo dirigido $G = (V, E)$ se dice \textbf{acíclico} si no contiene ningún ciclo dirigido.
A este tipo de estructuras se las conoce universalmente por sus siglas en inglés: \textbf{DAG} (\textit{Directed Acyclic Graph}).
\end{defin}
\chapter{Modelos Causales Estructurales e Intervenciones}

\section{Modelo Estructural Causal}
\subsection{Definición de SCM}
De manera intuitiva, es útil contar con variables aleatorias para definir relaciones causales, pues podemos expresarlas como transformaciones numéricas. Surge de manera natural tratar causas entre variables aleatorias como dependencias por relaciones funcionales. Así pues, diremos que una variable está causada por otra \textbf{si existe una relación funcional estructurada} entre ambas.

\begin{defin}[Ecuación estructural]
Sea $\text{PA}_i = \{P_1, P_2, \ldots, P_n\}$ un conjunto de variables aleatorias. Sea $\text{U}_i = \{U_1, U_2, \ldots, U_n\}$ otro conjunto de variables aleatorias.
Sea $X_i$ una variable aleatoria. Una \textbf{ecuación estructural} es aquella que toma la forma
\begin{equation*}
X_i := f_i(\text{PA}_i, \text{U}_i) = f_i(P_1, \ldots, P_n, U_1, \ldots, U_n)
\end{equation*}

Y diremos que $f_i$ es la ecuación que determina el valor de $X_i$ dados $\text{PA}_i$ y $\text{U}_i$. De manera coloquial, diremos que las variables de $\text{PA}_i$ causan el valor de $X_i$ con factores externos $U_i$.
\label{def:eq-estructural}
\end{defin}

Aquí utilizamos dos conjuntos de variables distintos. No es coincidencia haber escogido $\text{PA}$ para denotar las variables que `causan' el valor de $X_i$, pues se alinean con la Definición~\ref{def:parentesco}, con un matiz que veremos más adelante. Estas son las variables que tenemos en cuenta en nuestros modelos causales. Utilizamos el conjunto de variables $U$ para denotar a variables generadas por factores externos a lo que incluimos en nuestro modelo. Es entonces natural definir ahora el `puente' a los \textbf{modelos causales estructurales}. Diremos que existe una \textbf{relación funcional} entre un nodo y sus padres. Notemos que utilizamos $:=$ puesto que $X_i$ es una consecuencia de los valores de los que depende, pero los valores de los que depende no cambian si cambia $X_i$.

\begin{defin}[Estructura causal]
Diremos que una estructura causal de un conjunto de variables $V$ es un DAG en el que cada nodo está asociado a una variable de $V$, y cada arista representa una relación funcional entre un nodo y sus padres.
\end{defin}

\begin{defin}[Modelo causal o modelo estructural causal (SCM)]
Un modelo causal es un triple
\begin{equation*}
    M = \langle U, V, F\rangle
\end{equation*}
donde
\begin{enumerate}
    \item $U$ es un conjunto de variables sin otras que apunten a ellas. Por así decirlo, son `incausadas', o su causa no está contemplada dentro del modelo. Son llamadas `predeterminadas' o `exógenas'.
    \item $V$ es un conjunto de variables endógenas $\{V_1, \ldots, V_n\}$, es decir, que su causa se contempla dentro del modelo. Su causa viene determinada por variables en $V \cup U$.
    \item $F$ es un conjunto de ecuaciones estructurales $\{f_1, f_2, \ldots, f_n\}$ de acuerdo a lo definido en \ref{def:eq-estructural}. Cada $f_i$ determina el valor de $V_i$ de acuerdo a los conjuntos $\text{PA}_i$ y $U_i$ definidos. En este contexto, $\text{PA}_i \subseteq (V - \{V_i\}) $ es el conjunto de variables endógenas de las que depende $V_i$, mientras que $U_i \subseteq U$ es el conjunto de variables exógenas de las que depende $V_i$.
\end{enumerate}
\end{defin}
Aquí vemos la distinción entre $\text{PA}_i$ y lo que definimos en \ref{def:parentesco}. En realidad, en el contexto de un modelo estructural, $\text{PA}_i = \text{PA}(V_i) \cap V$.

\subsection{Grafo asociado a un Modelo Estructural Causal}
Como ya veníamos anticipando, podemos transformar el modelo estructural causal en su grafo correspondiente. Sea $M = \langle U, V, F\rangle$ un SCM. Definimos su estructura causal asociada (o DAG asociado) como $G = \langle V \cup U,\bigcup_{i\in \{1, \ldots,n \}} E_i \rangle$, donde $E_i = \{(C, V_i) \mid C \in \text{PA}_i \cup U_i\}$. Es decir, el grafo donde cada enlace representa una relación funcional estructural entre variables.

De acuerdo a la definición de SCM proporcionada, no tendríamos por qué suponer que en el grafo asociado no existen ciclos. Sin embargo, como modelamos causas, una intuición física nos diría que una causa \textbf{precede} a su efecto. Si hubiera un ciclo en el grafo asociado, tendríamos una relación temporal incompatible con lo que modelamos. Por ello, siempre hablaremos de de modelos \textit{Markovianos} en terminología de Pearl. En ellos, siempre existe un orden parcial de las variables endógenas tal que cada $V_i$ es consecuencia de sus precedentes en el orden.

Digamos que queremos representar, por medio de un SCM, cómo coexisten el estado de un interruptor ($X$), el estado de su circuito eléctrico asociado ($Y$) y el estado de la bombilla a la que conduce ($Z$). Defimimos también ls variables $U_X, U_Y$ y $U_Z$, que indican si ha ocurrido algo anómalo, o directamente si ocurre algún evento cuya causa no nos interesa modelar.

Dicho de otra forma, estamos considerando el SCM con $V = \{X, Y, Z\}$ y $U = \{U_X, U_Y, U_Z\}$. Para sus relaciones causales, tenemos que:

\begin{align*}
f_X : X &= U_X \\[1.5ex]
f_Y : Y &= \begin{cases} 
1 & \text{si } (X = 1 \wedge U_Y = 0) \vee (X = 0 \wedge U_Y = 1) \\ 
0 & \text{en cualquier otro caso} 
\end{cases} \\[1.5ex]
f_Z : Z &= \begin{cases} 
1 & \text{si } (Y = 1 \wedge U_Z = 0) \vee (Y = 0 \wedge U_Z = 1) \\ 
0 & \text{en cualquier otro caso} 
\end{cases}
\end{align*}

Transformando las funciones en relaciones causales de acuerdo al proceso de construcción de grafos que describimos más arriba, podemos construir el grafo de la Figura~\ref{fig:grafo-interruptor}. Si bien este grafo da las relaciones causales descritas anteriormente, notamos que las naturalezas de las relaciones causales \textbf{bien podrían ser distintas}, así como la naturaleza de las variables que manejamos.  

\begin{figure}[ht]
    \centering
\begin{tikzpicture}[
    node distance=2.5cm,
    decoration={markings, mark=at position 1 with {\arrow{stealth}}},
    every node/.style={circle, draw, minimum size=1cm, thick, fill=white}
]

% Nodos
\node (X) {$X$};
\node[right=of X] (Y) {$Y$};
\node[right=of Y] (Z) {$Z$};

\node[below left=of X](UX) {$U_X$};
\node[right=of UX] (UY) {$U_Y$};
\node[right=of UY] (UZ) {$U_Z$};

% Aristas
\draw[thick, postaction={decorate}] (X) -- (Y);
\draw[thick, postaction={decorate}] (Y) -- (Z);
\draw[thick, postaction={decorate}] (UY) -- (Y);
\draw[thick, postaction={decorate}] (UX) -- (X);
\draw[thick, postaction={decorate}] (UZ) -- (Z);

\end{tikzpicture}
\label{fig:grafo-interruptor}
\caption{Modelo estructural del SCM descrito. Las variables exógenas junto con las endógenas antecesoras de cada variable determinan el estado del sistema.}
\end{figure}

Supongamos, por ejemplo, que interpretamos $X$ como `número de minutos tarde que transcurren al despertarse', $Y$ como `número de minutos tarde que llegas a la oficina' y $Z$ como `duración del enfado de un jefe poco permisivo'. Las expresiones concretas que tuvieran las relaciones causales que existen entre estas tres variables seguirían sin influir en el grafo. Por tanto, la estructura causal de este modelo causal sería también la de la Figura~\ref{fig:grafo-interruptor}. Por tanto, si podemos decir algo del modelo gráfico, \textbf{podremos decirlo también de cualquier SCM} cuyo modelo gráfico sea ese.

\section{Estructuras topológicas elementales: cadena, bifurcación y colisión}
Supongamos un grafo dirigido de tres nodos $(X, Y, Z)$ tal que $X$ está conectado con $Z$, cualquiera que sea la dirección de dicha conexión. De forma similar, $Z$ está conectado con $Y$. Entonces, existen cuatro estados posibles.
\begin{enumerate}
    \item $X \rightarrow Z \rightarrow Y$ (o $X \leftarrow Z \leftarrow Y$, pero nos referimos a la anterior pues son simétricas), la \textbf{cadena}
    \item $X \leftarrow Z \rightarrow Y$, la \textbf{bifurcación}
    \item $X \rightarrow Z \leftarrow Y$, la \textbf{colisión}
\end{enumerate}

Podríamos aventurarnos a formular afirmaciones sobre la dependencia de las variables. En el caso de la cadena, la intuición nos dice que $Z$ e $Y$ son dependientes, es decir $P(Z = z \mid Y = y) \neq P(Z)$ para algún par $z, y$. Podríamos también aventurarnos a afirmar que $Z$ y $X$ son dependientes. Podemos construir ejemplos `artificiales' donde estas afirmaciones sean falsas (ver Apéndice~\ref{app:independent}), pero no buscamos que se cumplan rigurosamente.

Lo que sí podemos afirmar es lo siguiente

\begin{lema}
    aa
\end{lema}

\clearpage

\appendix 
\chapter{Demostración de la Ley de Probabilidad Total}
\label{app:demo}

\begin{proof}
Como $A \subseteq \Omega$,
\begin{equation*}
    A = A \cap \Omega = A \cap \left(\bigcup_{i \in I} B_i\right)
\end{equation*}
Por la propiedad distributiva de la unión y la intersección, tenemos que 
\begin{equation*}
A \cap \left(\bigcup_{i \in I} B_i\right) = \bigcup_{i \in I} (B_i \cap A)
\end{equation*}

Aplicando ahora el axioma de la $\sigma-$aditividad de $P$ (utilizamos que los conjuntos $B_i$ son una partición de $\Omega$), tenemos que 

\begin{equation*}
P(A) = P\left(\bigcup_{i \in I} (B_i \cap A)\right) = \sum_{i \in I} P(B_i \cap A)
\end{equation*}

Aplicando ahora la definición de probabilidad condicionada, llegamos al resultado deseado:

\begin{equation*}
P(A) = \sum_{i \in I} P(B_i \cap A) = \sum_{i \in I} P(A \mid B_i)P(B_i)
\end{equation*}

\end{proof}

\chapter{Independencia en la cadena}
\label{app:independent}

Supongamos tres variables aleatorias $X, Y, Z$ conectadas por un SCM en cadena tal que $X \rightarrow Y \rightarrow Z$. Sabemos entonces que existen $f_Y, f_Z$ tales que $f_Y$ determina el valor de $Y$ dependiente del valor de $X$ y análogamente con $f_Z$, $Z$ e $Y$.

Suponemos que estamos en un espacio de probabilidad $(\Omega, \mathcal{F}, P)$. Si definimos $h = f_Y \circ f_Z$ tenemos la relación causal entre $X$ y $Z$. Si suponemos variables exógenas $U_Y, U_Z$, tenemos


\printbibliography

\cleardoublepage


\end{document}
